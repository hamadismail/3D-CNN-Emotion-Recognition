{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5583070,"sourceType":"datasetVersion","datasetId":3213010},{"sourceId":6331588,"sourceType":"datasetVersion","datasetId":3644455}],"dockerImageVersionId":30407,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Data Preparation","metadata":{}},{"cell_type":"code","source":"import os\nRoot = \"/kaggle/input/ravdess/\"\nos.chdir(Root)","metadata":{"id":"4CKPHhxqoeid","outputId":"93c783a3-53c2-4bf9-a8ce-1d8c12042df5","execution":{"iopub.status.busy":"2023-11-05T07:37:43.327346Z","iopub.execute_input":"2023-11-05T07:37:43.327747Z","iopub.status.idle":"2023-11-05T07:37:43.333302Z","shell.execute_reply.started":"2023-11-05T07:37:43.327711Z","shell.execute_reply":"2023-11-05T07:37:43.332084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing Libraries","metadata":{}},{"cell_type":"code","source":"import librosa\nimport soundfile\nimport os, glob, pickle\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score","metadata":{"id":"_IehQoF0pZxl","execution":{"iopub.status.busy":"2023-11-05T07:37:43.335023Z","iopub.execute_input":"2023-11-05T07:37:43.335314Z","iopub.status.idle":"2023-11-05T07:37:43.771244Z","shell.execute_reply.started":"2023-11-05T07:37:43.335286Z","shell.execute_reply":"2023-11-05T07:37:43.770162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##  <center> 1. Ravdess Dataframe <center>\n","metadata":{}},{"cell_type":"code","source":"ravdess_directory_list = os.listdir(Root)\n\nfile_emotion = []\nfile_path = []\nfor dir in ravdess_directory_list:\n    # as their are 20 different actors in our previous directory we need to extract files for each actor.\n    actor = os.listdir(Root + dir)\n    for file in actor:\n        part = file.split('.')[0]\n        part = part.split('-')\n        # third part in each file represents the emotion associated to that file.\n        file_emotion.append(int(part[2]))\n        file_path.append(Root + dir + '/' + file)\n        \n# dataframe for emotion of files\nemotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n\n# dataframe for path of files.\npath_df = pd.DataFrame(file_path, columns=['Path'])\nRavdess_df = pd.concat([emotion_df, path_df], axis=1)\n\n# changing integers to actual emotions.\nRavdess_df.Emotions.replace({1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'}, inplace=True)\nRavdess_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-05T07:37:43.773388Z","iopub.execute_input":"2023-11-05T07:37:43.773773Z","iopub.status.idle":"2023-11-05T07:37:44.363519Z","shell.execute_reply.started":"2023-11-05T07:37:43.773736Z","shell.execute_reply":"2023-11-05T07:37:44.362507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating Dataframe using ravdees dataframes we created so far.\ndata_path = pd.concat([Ravdess_df], axis = 0)\ndata_path.to_csv(\"/kaggle/working/data_path.csv\",index=False)\n# data_path.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-05T07:37:44.364799Z","iopub.execute_input":"2023-11-05T07:37:44.365092Z","iopub.status.idle":"2023-11-05T07:37:44.377793Z","shell.execute_reply.started":"2023-11-05T07:37:44.365064Z","shell.execute_reply":"2023-11-05T07:37:44.376922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def noise(data):\n    noise_amp = 0.035*np.random.uniform()*np.amax(data)\n    data = data + noise_amp*np.random.normal(size=data.shape[0])\n    return data\n\ndef stretch(data, rate=0.8):\n    return librosa.effects.time_stretch(data, rate=rate)\n\ndef shift(data):\n    shift_range = int(np.random.uniform(low=-5, high = 5)*1000)\n    return np.roll(data, shift_range)\n\ndef pitch(data, sr=22050, n_steps=2):\n    return librosa.effects.pitch_shift(data, sr=sr, n_steps=n_steps)\n\n# taking any example and checking for techniques.\npath = np.array(data_path.Path)[1]\ndata, sample_rate = librosa.load(path)","metadata":{"execution":{"iopub.status.busy":"2023-11-05T07:37:44.380227Z","iopub.execute_input":"2023-11-05T07:37:44.380544Z","iopub.status.idle":"2023-11-05T07:37:53.914491Z","shell.execute_reply.started":"2023-11-05T07:37:44.380515Z","shell.execute_reply":"2023-11-05T07:37:53.913342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_features(data):\n\n    # Chroma_stft\n    stft = np.abs(librosa.stft(data))\n    chroma_stft = librosa.feature.chroma_stft(S=stft, sr=sample_rate, n_chroma=64)\n    # If the resulting Chroma_stft matrix has fewer frames than n_frames, you can pad it.\n    if chroma_stft.shape[1] < 108:\n        pad_width = 108 - chroma_stft.shape[1]\n        chroma_stft = np.pad(chroma_stft, ((0, 0), (0, pad_width)), mode='constant')\n\n    # If the resulting Chroma_stft matrix has more frames than n_frames, you can truncate it.\n    if chroma_stft.shape[1] > 108:\n        chroma_stft = chroma_stft[:, :108]\n    \n    \n    # MFCC\n    mfccs = librosa.feature.mfcc(y=data, sr=sample_rate, n_mfcc=64)\n    # If the resulting MFCC matrix has fewer frames than n_frames, you can pad it.\n    if mfccs.shape[1] < 108:\n        pad_width = 108 - mfccs.shape[1]\n        mfccs = np.pad(mfccs, ((0, 0), (0, pad_width)), mode='constant')\n\n    # If the resulting MFCC matrix has more frames than n_frames, you can truncate it.\n    if mfccs.shape[1] > 108:\n        mfccs = mfccs[:, :108]\n          \n    # MelSpectogram\n    mel = librosa.feature.melspectrogram(y=data, sr=sample_rate, n_mels=64)\n    # If the resulting MelSpectogram matrix has fewer frames than n_frames, you can pad it.\n    if mel.shape[1] < 108:\n        pad_width = 108 - mel.shape[1]\n        mel = np.pad(mel, ((0, 0), (0, pad_width)), mode='constant')\n\n    # If the resulting MelSpectogram matrix has more frames than n_frames, you can truncate it.\n    if mel.shape[1] > 108:\n        mel = mel[:, :108]\n        \n    result = np.stack((chroma_stft, mfccs,mel)) # stacking horizontally \n    \n    return result\n\ndef get_features(path):\n    # duration and offset are used to take care of the no audio in start and the ending of each audio files as seen above.\n    data, sample_rate = librosa.load(path, duration=2.5, offset=0.6)\n    \n    # without augmentation\n    res1 = extract_features(data) # 13\n    result = np.array(res1) # (13, )\n\n    # data with noise\n    noise_data = noise(data)\n    res2 = extract_features(noise_data) # (13, )\n    result = np.vstack((result, res2)) # stacking vertically (2, 13)\n    \n    result = result.reshape(2, 64, 108, 3)\n    \n    return result","metadata":{"execution":{"iopub.status.busy":"2023-11-05T07:37:53.924526Z","iopub.execute_input":"2023-11-05T07:37:53.924796Z","iopub.status.idle":"2023-11-05T07:37:53.940337Z","shell.execute_reply.started":"2023-11-05T07:37:53.924769Z","shell.execute_reply":"2023-11-05T07:37:53.939170Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X, Y = [], []\n\nfor path, emotion in zip(data_path.Path, data_path.Emotions):\n    feature = get_features(path)\n\n    for ele in feature:\n        X.append(ele)\n        # appending emotion 3 times as we have made 3 augmentation techniques on each audio file.\n        Y.append(emotion)","metadata":{"execution":{"iopub.status.busy":"2023-11-05T07:37:53.941605Z","iopub.execute_input":"2023-11-05T07:37:53.941984Z","iopub.status.idle":"2023-11-05T07:40:59.736830Z","shell.execute_reply.started":"2023-11-05T07:37:53.941956Z","shell.execute_reply":"2023-11-05T07:40:59.735171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(feature.shape)\nprint(ele.shape)","metadata":{"execution":{"iopub.status.busy":"2023-11-05T07:40:59.739002Z","iopub.execute_input":"2023-11-05T07:40:59.739814Z","iopub.status.idle":"2023-11-05T07:40:59.746562Z","shell.execute_reply.started":"2023-11-05T07:40:59.739753Z","shell.execute_reply":"2023-11-05T07:40:59.745474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = np.array(X)\nY = np.array(Y)\n\nprint(X.shape, Y.shape, len(X), len(Y))","metadata":{"execution":{"iopub.status.busy":"2023-11-05T07:40:59.748519Z","iopub.execute_input":"2023-11-05T07:40:59.749377Z","iopub.status.idle":"2023-11-05T07:40:59.941888Z","shell.execute_reply.started":"2023-11-05T07:40:59.749336Z","shell.execute_reply":"2023-11-05T07:40:59.940872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_samples = len(X)\nnum_frequency_bins = X[0].shape[0]\nnum_time_steps = X[0].shape[1]\nnum_fea_type = X[0].shape[2]\n\nprint(\"Number of samples:\", num_samples)\nprint(\"Number of frequency bins:\", num_frequency_bins)\nprint(\"Number of time steps:\", num_time_steps)\nprint(\"Number of feature_method:\", num_fea_type)","metadata":{"execution":{"iopub.status.busy":"2023-11-05T07:40:59.945800Z","iopub.execute_input":"2023-11-05T07:40:59.946182Z","iopub.status.idle":"2023-11-05T07:40:59.952916Z","shell.execute_reply.started":"2023-11-05T07:40:59.946150Z","shell.execute_reply":"2023-11-05T07:40:59.951848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Flatten the 3D feature array into 2D\nX_flattened = X.reshape(X.shape[0], -1)\n\nFeatures = pd.DataFrame(X_flattened)\nFeatures['labels'] = Y\nFeatures.to_csv('/kaggle/working/features.csv', index=False)\nFeatures.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-05T07:40:59.953981Z","iopub.execute_input":"2023-11-05T07:40:59.954294Z","iopub.status.idle":"2023-11-05T07:43:16.742379Z","shell.execute_reply.started":"2023-11-05T07:40:59.954267Z","shell.execute_reply":"2023-11-05T07:43:16.741355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preparation\n\n- As of now we have extracted the data, now we need to normalize and split our data for training and testing.","metadata":{}},{"cell_type":"code","source":"# X = Features.iloc[: ,:-1].values\nX = np.array(Features.iloc[:, :-1].values).reshape(num_samples, num_frequency_bins, num_time_steps, num_fea_type)\nY = Features['labels'].values","metadata":{"execution":{"iopub.status.busy":"2023-11-05T07:43:16.743848Z","iopub.execute_input":"2023-11-05T07:43:16.744609Z","iopub.status.idle":"2023-11-05T07:43:17.183671Z","shell.execute_reply.started":"2023-11-05T07:43:16.744565Z","shell.execute_reply":"2023-11-05T07:43:17.182767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n# As this is a multiclass classification problem onehotencoding our Y.\nencoder = OneHotEncoder()\nY = encoder.fit_transform(np.array(Y).reshape(-1,1)).toarray()","metadata":{"execution":{"iopub.status.busy":"2023-11-05T07:43:17.185065Z","iopub.execute_input":"2023-11-05T07:43:17.185955Z","iopub.status.idle":"2023-11-05T07:43:17.193873Z","shell.execute_reply.started":"2023-11-05T07:43:17.185906Z","shell.execute_reply":"2023-11-05T07:43:17.192619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# splitting data\nx_train, x_test, y_train, y_test = train_test_split(X, Y, random_state=0, test_size=0.2, shuffle=True)\nx_train.shape, y_train.shape, x_test.shape, y_test.shape","metadata":{"execution":{"iopub.status.busy":"2023-11-05T07:43:17.195284Z","iopub.execute_input":"2023-11-05T07:43:17.195709Z","iopub.status.idle":"2023-11-05T07:43:17.315604Z","shell.execute_reply.started":"2023-11-05T07:43:17.195678Z","shell.execute_reply":"2023-11-05T07:43:17.314597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n# Reshape the data to 2D\nx_train_2d = x_train.reshape(x_train.shape[0], -1)\nx_test_2d = x_test.reshape(x_test.shape[0], -1)\n\n# Apply StandardScaler to the 2D data\nscaler = StandardScaler()\nx_train_scaled = scaler.fit_transform(x_train_2d)\nx_test_scaled = scaler.transform(x_test_2d)\n\n# Reshape the scaled data back to 3D\nx_train = x_train_scaled.reshape(x_train.shape[0], num_frequency_bins, num_time_steps, num_fea_type)\nx_test = x_test_scaled.reshape(x_test.shape[0], num_frequency_bins, num_time_steps, num_fea_type)\n\nx_train.shape, y_train.shape, x_test.shape, y_test.shape\n","metadata":{"execution":{"iopub.status.busy":"2023-11-05T07:43:17.316879Z","iopub.execute_input":"2023-11-05T07:43:17.317182Z","iopub.status.idle":"2023-11-05T07:43:17.993238Z","shell.execute_reply.started":"2023-11-05T07:43:17.317152Z","shell.execute_reply":"2023-11-05T07:43:17.992047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modelling","metadata":{}},{"cell_type":"code","source":"import keras\nfrom keras.layers import Input, Conv3D, MaxPooling3D, Flatten, Dense, Dropout, BatchNormalization\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Model, Sequential\nimport numpy as np\n\n# Define the model\nmodel = Sequential()\n\n# First Conv3D layer\nmodel.add(Conv3D(64, kernel_size=(3, 3, 3), strides=(1, 1, 1), padding='same', activation='relu', input_shape=(num_frequency_bins, num_time_steps, num_fea_type, 1)))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 1)))\n\n# Second Conv3D layer\nmodel.add(Conv3D(128, kernel_size=(3, 3, 3), strides=(1, 1, 1), padding='same', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling3D(pool_size=(2, 2, 1), strides=(2, 2, 1)))\n\n# Second Conv3D layer\nmodel.add(Conv3D(128, kernel_size=(3, 3, 3), strides=(1, 1, 1), padding='same', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling3D(pool_size=(2, 2, 1), strides=(2, 2, 1)))\n\n# Third Conv3D layer\nmodel.add(Conv3D(256, kernel_size=(3, 3, 3), strides=(1, 1, 1), padding='same', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling3D(pool_size=(2, 2, 1), strides=(2, 2, 1)))\n\n# Flatten layer\nmodel.add(Flatten())\n\n# Fully connected layers\nmodel.add(Dense(1000, activation='relu'))\n# model.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(8, activation='softmax'))\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Print model summary\nmodel.summary()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-05T09:45:08.342204Z","iopub.execute_input":"2023-11-05T09:45:08.342914Z","iopub.status.idle":"2023-11-05T09:45:08.559686Z","shell.execute_reply.started":"2023-11-05T09:45:08.342873Z","shell.execute_reply":"2023-11-05T09:45:08.558657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ReduceLROnPlateau\n\nrlrp = ReduceLROnPlateau(monitor='loss', factor=0.4, verbose=0, patience=2, min_lr=0.0000001)\nhistory=model.fit(x_train, y_train, batch_size=64, epochs=100, validation_data=(x_test, y_test), callbacks=[rlrp])","metadata":{"execution":{"iopub.status.busy":"2023-11-05T09:45:24.875060Z","iopub.execute_input":"2023-11-05T09:45:24.875793Z","iopub.status.idle":"2023-11-05T10:02:01.406197Z","shell.execute_reply.started":"2023-11-05T09:45:24.875753Z","shell.execute_reply":"2023-11-05T10:02:01.405118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nprint(\"Accuracy of our model on test data : \" , model.evaluate(x_test,y_test)[1]*100 , \"%\")\n\nepochs = [i for i in range(100)]\nfig , ax = plt.subplots(1,2)\ntrain_acc = history.history['accuracy']\ntrain_loss = history.history['loss']\ntest_acc = history.history['val_accuracy']\ntest_loss = history.history['val_loss']\n\nfig.set_size_inches(20,6)\nax[0].plot(epochs , train_loss , label = 'Training Loss')\nax[0].plot(epochs , test_loss , label = 'Testing Loss')\nax[0].set_title('Training & Testing Loss')\nax[0].legend()\nax[0].set_xlabel(\"Epochs\")\n\nax[1].plot(epochs , train_acc , label = 'Training Accuracy')\nax[1].plot(epochs , test_acc , label = 'Testing Accuracy')\nax[1].set_title('Training & Testing Accuracy')\nax[1].legend()\nax[1].set_xlabel(\"Epochs\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-05T08:07:26.261964Z","iopub.status.idle":"2023-11-05T08:07:26.262539Z","shell.execute_reply.started":"2023-11-05T08:07:26.262310Z","shell.execute_reply":"2023-11-05T08:07:26.262334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predicting on test data.\npred_test = model.predict(x_test)\ny_pred = encoder.inverse_transform(pred_test)\n\ny_test = encoder.inverse_transform(y_test)","metadata":{"execution":{"iopub.status.busy":"2023-11-05T08:07:26.264064Z","iopub.status.idle":"2023-11-05T08:07:26.264489Z","shell.execute_reply.started":"2023-11-05T08:07:26.264257Z","shell.execute_reply":"2023-11-05T08:07:26.264283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(columns=['Predicted Labels', 'Actual Labels'])\ndf['Predicted Labels'] = y_pred.flatten()\ndf['Actual Labels'] = y_test.flatten()\n\ndf.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-11-05T08:07:26.266010Z","iopub.status.idle":"2023-11-05T08:07:26.266376Z","shell.execute_reply.started":"2023-11-05T08:07:26.266194Z","shell.execute_reply":"2023-11-05T08:07:26.266212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\n\ncm = confusion_matrix(y_test, y_pred)\n# plt.figure(figsize = (12, 10))\ncm = pd.DataFrame(cm , index = [i for i in encoder.categories_] , columns = [i for i in encoder.categories_])\nsns.heatmap(cm, linecolor='white', cmap='Blues', linewidth=1, annot=True, fmt='')\nplt.title('Confusion Matrix', size=20)\nplt.xlabel('Predicted Labels', size=14)\nplt.ylabel('Actual Labels', size=14)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-05T08:07:26.267574Z","iopub.status.idle":"2023-11-05T08:07:26.267926Z","shell.execute_reply.started":"2023-11-05T08:07:26.267743Z","shell.execute_reply":"2023-11-05T08:07:26.267760Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_pred, zero_division=0))","metadata":{"execution":{"iopub.status.busy":"2023-11-05T08:07:26.269046Z","iopub.status.idle":"2023-11-05T08:07:26.269416Z","shell.execute_reply.started":"2023-11-05T08:07:26.269229Z","shell.execute_reply":"2023-11-05T08:07:26.269249Z"},"trusted":true},"execution_count":null,"outputs":[]}]}